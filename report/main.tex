% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{MAIN}
\usepackage[]{MAIN}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

% Used for including images
\usepackage{graphicx}

% Better visual effect of hyper-links
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{color}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% Show header and footer
\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{} % Clear existing header and footer
\fancyfoot[C]{\thepage} % Place the page number in the center of the footer

% Math
\usepackage{amsmath}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Community Detection and Analysis on DBLP v9 Dataset: Centrality, Network Metrics, and Predictive Modeling}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{
%   Author 1 \and ... \and Author n \\
%   Address line \\ ... \\ Address line
% }
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
\author{
  Xiang Zheng \\ 21307110169 \And  
  Qin Ma      \\ 21307110024 
  \vspace{0.2cm} \\ % Add some space before the school name
  \hfill School of Data Science, Fudan University \hfill
  \And
  Author n \\ Address line
}

\begin{document}
\maketitle
\begin{abstract}
	abstract
\end{abstract}

\section{Introduction}

Network analysis is a critical methodology for studying complex systems across various domains. The \href{https://www.aminer.cn/citation}{DBLP dataset}, a comprehensive bibliographic resource for computer science, forms the foundation for investigating collaboration patterns and scholarly relationships. This study applies advanced graph analysis techniques to explore the structural and functional properties of the DBLP v9 dataset.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.48\textwidth]{img/introduction/visualization.png}
	\caption{Snapshot of the visualization system}
	\label{fig:visualize}
\end{figure}

The subsequent sections of this report follow the analytical framework outlined below:
\\

\textbf{Preprocessing}: Transforming the raw DBLP data into a structured network through cleaning, filtering, and attribute assignment.

\textbf{Community Detection}: Identifying cohesive subgroups using algorithms like Louvain and Label Propagation to uncover collaboration dynamics and research specializations.

\textbf{Centrality Analysis}: Analyzing metrics such as degree centrality and PageRank to identify influential nodes and assess network topology.
\textbf{Link Prediction}: Employing the GLACE model on the Cora-ML dataset to predict future connections based on structural features, providing insights into evolving citation trends.

\textbf{Visualization}: Presenting findings through an interactive system developed with \textbf{Python} and \textbf{D3.js}, highlighting key network characteristics and community structures.
\\

This structured approach provides a comprehensive examination of the DBLP co-authorship and publication network, delivering insights into collaboration patterns, influential figures, and the evolution of academic networks.



\section{Preprocessing}

The preprocessing phase is a critical step in transforming the raw DBLP V9 dataset into a structured format suitable for detailed analysis. This stage encompasses data cleaning, network construction, feature engineering, and dataset filtering. The DBLP V9 dataset was specifically chosen for its balance between computational feasibility and data comprehensiveness, enabling robust analysis of academic collaboration and citation patterns within computer science.

\subsection{Dataset Selection}

The DBLP-Citation-network V9 dataset was selected due to its optimal balance between data richness and computational manageability, as summarized in Table \ref{tab:dblp_citation_networks}. Other versions of the DBLP dataset, while valuable, either lacked the depth of information or exceeded practical computational limits for this project. DBLP V9 captures key trends up to July 3, 2017, offering a comprehensive yet tractable dataset with \textbf{3,680,007 papers} and \textbf{1,876,067 citation relationships}.

\begin{table*}[h!]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Dataset Version}          & \textbf{Number of Papers} & \textbf{Number of Citation Relationships} \\ \hline
		Citation-network V1               & 629,814                   & >632,752                                  \\ \hline
		Citation-network V2               & 1,397,240                 & >3,021,489                                \\ \hline
		DBLP-Citation-network V3          & 1,632,442                 & >2,327,450                                \\ \hline
		DBLP-Citation-network V4          & 1,511,035                 & 2,084,019                                 \\ \hline
		DBLP-Citation-network V5          & 1,572,277                 & 2,084,019                                 \\ \hline
		DBLP-Citation-network V6          & 2,084,055                 & 2,244,018                                 \\ \hline
		DBLP-Citation-network V7          & 2,244,021                 & 4,354,534                                 \\ \hline
		DBLP-Citation-network V8          & 3,272,991                 & 8,466,859                                 \\ \hline
		\textbf{DBLP-Citation-network V9} & \textbf{3,680,007}        & \textbf{1,876,067}                        \\ \hline
		DBLP-Citation-network V10         & 3,079,007                 & 25,166,994                                \\ \hline
		DBLP-Citation-network V11         & 4,107,340                 & 36,624,464                                \\ \hline
		DBLP-Citation-network V12         & 4,894,081                 & 45,564,149                                \\ \hline
		DBLP-Citation-network V13         & 5,354,309                 & 48,227,950                                \\ \hline
		DBLP-Citation-network V14         & 5,259,858                 & 36,630,661                                \\ \hline
	\end{tabular}
	\caption{Summary of DBLP Citation Network Versions}
	\label{tab:dblp_citation_networks}
\end{table*}

\subsection{Data Cleaning and Integration}

The raw DBLP dataset includes metadata such as titles, authors, venues, and citations. Data cleaning involved:
\begin{itemize}
	\item \textbf{Grouping Records:} Papers were grouped using unique identifiers to ensure each record was correctly structured.
	\item \textbf{Resolving Missing Data:} Missing fields, such as titles, authors, or venues, were assigned default values to maintain consistency.
\end{itemize}

These steps ensured the dataset was standardized and ready for subsequent analysis.

\subsection{Network Construction}

The dataset was represented as two interconnected networks:

\begin{enumerate}
	\item \textbf{Co-authorship Network:} In this network, authors are represented as nodes, with weighted edges denoting co-authorship relationships. The weight of each edge corresponds to the frequency of collaborations between authors. This network contains a total of 3,680,007 nodes (authors) and 1,876,067 edges (co-authorship relationships).

	\item \textbf{Citation Network:} The citation network is constructed by representing papers as nodes, with directed edges indicating citation relationships between them. Each edge direction signifies the citing paper and the cited paper. This network also comprises 3,680,007 nodes (papers) and 1,876,067 edges (citation relationships).
\end{enumerate}


This dual representation allows for a comprehensive study of collaboration and citation dynamics.

\subsection{Feature Engineering}

Key features were engineered to enhance the datasetâ€™s analytical capabilities:
\begin{itemize}
	\item \textbf{Co-authorship Features:} Unique author identifiers were assigned, and collaboration frequencies were calculated.
	\item \textbf{Citation Metrics:} In-degree (citations received) and out-degree (references) were computed for each paper.
	\item \textbf{Venue Indexing:} Publication venues were standardized and indexed for uniform representation.
\end{itemize}

\subsection{Dataset Filtering}

Papers with no citations and references were flagged as "isolate" and excluded to improve computational efficiency. Additionally, thresholds were applied to focus on significant collaborations and impactful papers.

\subsection{Exploratory Analysis}

Exploratory analyses were conducted using Python libraries such as \texttt{pandas} and \texttt{matplotlib}. The key findings are visualized in Figures \ref{fig:num_authors_per_paper}--\ref{fig:num_coauthors_per_author}:

\begin{itemize}
	\item \textbf{Authors per Paper:} Most papers have few authors, with fewer multi-author publications.
	\item \textbf{Citation Distribution:} Citations are highly skewed, with a small number of papers receiving the majority of citations.
	\item \textbf{References per Paper:} Papers with more citations tend to reference more works.
	\item \textbf{Co-authors per Author:} A small group of authors collaborate extensively, while most have limited collaborations.
\end{itemize}

\begin{figure*}[htbp]
	\centering
	\begin{minipage}{0.24\textwidth}
		\includegraphics[width=\textwidth]{img/preprocess/num_authors.png}
		\caption{Number of Authors per Paper}
		\label{fig:num_authors_per_paper}
	\end{minipage} \hfill
	\begin{minipage}{0.24\textwidth}
		\includegraphics[width=\textwidth]{img/preprocess/num_citations.png}
		\caption{Citation Distribution of Papers}
		\label{fig:citation_distribution}
	\end{minipage} \hfill
	\begin{minipage}{0.24\textwidth}
		\includegraphics[width=\textwidth]{img/preprocess/num_references.png}
		\caption{Reference Distribution of Papers}
		\label{fig:reference_distribution}
	\end{minipage} \hfill
	\begin{minipage}{0.24\textwidth}
		\includegraphics[width=\textwidth]{img/preprocess/num_coauthors.png}
		\caption{Number of Co-authors per Author}
		\label{fig:num_coauthors_per_author}
	\end{minipage}
\end{figure*}

These analyses offer critical insights into academic collaboration and citation patterns, laying the groundwork for deeper exploration of academic network structures.



\section{Community Mining}
In academic networks, community mining aims to identify cohesive subgroups reflecting collaborative dynamics, research specializations, and the structural foundation of scholarly interactions. Three common algorithms - Louvain, Label Propagation, and Multi - level - are used to discover potential communities based on node relationships.

After detection, community proportions and modularity are calculated to assess the quality of community division. They provide insights into the network's structure, indicating whether detected communities have strong internal and weak external connections, characteristic of well - formed community structures. 

\subsection{Overview of Community Mining}
Community detections operate by grouping nodes in a graph based on their connectivity. In the context of academic networks, nodes represent entities such as papers or authors, and edges represent relationships between these entities, such as co-authorship or citations. Below are the specifics of Paper Networks and Author Networks.
\begin{itemize}
    \item {Paper Network:}In a paper network, nodes represent individual papers, and edges represent relationships such as citations or co-authorships. Edges are weighted by the number of citations or co-authors shared. Community detection aims to identify research areas or subfields where papers are closely linked by citations or co-authors, revealing thematic connections or shared academic topics.
    \item {Author Network:}In an author network, nodes represent authors, and edges represent co-authorships. The edge weight reflects the number of joint papers between authors, with higher weights indicating stronger collaboration. Community detection in this network uncovers collaborative groups or research teams, offering insights into research partnerships and collaboration patterns.
\end{itemize}

The goal of community detection is to find subsets of nodes (communities) where the internal connectivity is higher than expected by random chance, and the external connectivity is relatively lower. Mathematically, this is often expressed in terms of modularity.


\subsection{Community Detection Algorithms}

\subsubsection{Louvain Algorithm}
The Louvain algorithm is a method for detecting communities by optimizing modularity. 
The modularity \( Q \) for a community division is calculated as:

\[
Q = \frac{1}{2m} \sum_{i,j} \left( A_{ij} - \frac{k_i k_j}{2m} \right) \delta(c_i, c_j)
\]
Where: 
\( m \) is the total number of edges in the network.
\( A_{ij} \) is the weight of the edge between nodes \( i \) and \( j \).
\( k_i \) and \( k_j \) are the degrees of nodes \( i \) and \( j \).
\( \delta(c_i, c_j) \) is 1 if nodes \( i \) and \( j \) are in the same community and 0 otherwise. 
\( c_i \) and \( c_j \) are the community labels of nodes \( i \) and \( j \).

The Louvain algorithm works in two phases:

1. Local optimization: Each node is assigned to the community of its neighbor that maximizes modularity.

2. Community aggregation: Communities are treated as super-nodes, and the algorithm repeats the process of modularity optimization on this new, aggregated graph.

The algorithm is computationally efficient and is well-suited for large-scale networks, making it a popular choice in detecting academic collaboration communities.

\subsubsection{Label Propagation Algorithm}
Label Propagation (LP) is a simple and efficient algorithm where each node is initially assigned a unique label. At each iteration, each node updates its label to the most frequent label among its neighbors. This process continues until the labels stabilize.

Mathematically, Label Propagation is expressed as:

\[
l_i^{(t+1)} = \arg\max_{l_j \in N(i)} \sum_{k \in N(i), l_k = l_j} \frac{1}{d_k}
\]
Where: 
\( l_i^{(t+1)} \) is the new label for node 
\( i \) after iteration \( t \). \( N(i) \) is the set of neighbors of node \( i \).
\( d_k \) is the degree of neighbor \( k \).

The algorithm is highly parallelizable and does not require predefined parameters like the number of communities. It is particularly efficient for large networks with many nodes and edges.

\subsubsection{Multi-level Algorithm}
The Multi-level algorithm is based on a hierarchical approach, where the graph is coarsened into a smaller graph by iteratively merging nodes that are highly connected. The algorithm detects communities at multiple levels by optimizing modularity at each level. After the graph is coarsened, the community detection process is applied to the smaller graph, and the solution is refined by uncoarsing the graph back to its original size.

The Multi-level algorithm follows these general steps:

1. Coarsing: Repeatedly reduce the graph by merging highly connected nodes, creating a smaller graph at each level.

2. Community Detection: Perform community detection on the coarsed graph (typically using modularity maximization).

3. Uncoarsing: Refine the community structure by uncoarsing the graph and applying the community structure from the coarser level to the finer levels.

This method can efficiently handle large-scale networks and is suitable for discovering communities at different scales.

\subsubsection{Reasons for Choosing These Algorithms}
These three algorithms represent different approaches to community detection:
\begin{itemize}
    \item Louvain emphasizes modularity optimization and is efficient for large networks.
    \item Label Propagation uses local information propagation and is highly scalable and efficient.
    \item Multi-level works at multiple scales, ensuring that communities are detected at both coarse and fine levels.
\end{itemize}

Each algorithm brings unique strengths, making them well-suited for the diverse and complex academic networks that we are analyzing.


\subsection{Community Proportions}
The community proportions represent the distribution of nodes across different communities. By calculating the size of each community (i.e., the number of nodes within it) and dividing by the total number of nodes, we can get the proportion of each community in the overall network.

Let \( N_c \) be the number of nodes in community \( c \), and \( N_{\text{total}} \) be the total number of nodes in the network. The proportion of community \( c \) is given by:

\[
\text{Proportion of community } c = \frac{N_c}{N_{\text{total}}}
\]

This helps to understand the size and importance of each community within the entire network. Larger communities may represent more significant areas of academic collaboration, while smaller ones could represent more specialized or niche research areas.

\subsection{Modularity}

Modularity is a key measure used to evaluate the quality of community detection. It quantifies the strength of division of a network into communities by comparing the number of edges within communities to the number of edges expected by random chance.

The modularity \( Q \) is calculated using the formula mentioned earlier. A modularity value greater than 0 indicates that the community division is better than a random distribution of edges, with higher values indicating stronger community structures.

The following presents the modularity values obtained from community detection in the paper network using the Louvain, Label Propagation, and Multi-level algorithms in this study. 


\begin{table}
    \centering
    \begin{tabular}{|c|c|}
    \hline
    \textbf{Algorithm} & \textbf{Modularity} \\
    \hline
    Louvain  & 0.9939834022020653 \\
    Label Propagation  & 0.9909479458016406 \\
    Multi-level   &  0.9938523610510882 \\
    \hline
    \end{tabular}
    \caption{Modularity values for community detection in the paper network using different algorithms}
    \label{tab:my_label}
\end{table}

The results indicate that the modularity for all three algorithms exceeds 99\%, suggesting that the community partitions identified by these methods exhibit strong internal cohesion and weak external connections. This high modularity score reflects the algorithms' effectiveness in detecting well-structured communities with clear, well-defined boundaries, underscoring their suitability for analyzing academic collaboration networks. 


\subsection{Community Detection Results}
The results of community detection in paper network are as follows: using the Louvain algorithm, a total of 29,898 communities were detected; the Label Propagation algorithm identified 30,549 communities; and the Multi-level algorithm found 29,589 communities. The following figures illustrate the community partitioning of the paper network using the Louvain algorithm.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/community.jpg}
    \caption{Community partitioning of the paper network using the Louvain algorithm}
    \label{fig:community}
\end{figure}

In these visualizations, nodes represent individual papers, and edges represent relationships such as citations or co-authorships. Different colors indicate distinct communities within the network. These visualizations offer a clear representation of how papers are grouped into cohesive subgroups, reflecting collaborative research themes or areas of study.



\section{Centrality Measurement}

After detecting communities within the academic network, it is essential to assess the importance and influence of individual nodes within each community. In this analysis, centrality measures such as Degree Centrality and PageRank Centrality were calculated to identify influential nodes, revealing key hubs and authoritative figures within the network. Additionally, structural characteristics of the network, including community diameters and average citations per author, were analyzed to understand the overall network topology and functional properties. The following sections describe the methodology and results of these calculations:

\subsection{Centrality Calculation}

\subsubsection{Degree Centrality}

This measure counts the number of direct connections (edges) a node has, reflecting its immediate popularity or connectivity within the network. A higher degree centrality indicates a node that is well-connected, potentially influencing many other nodes within the community. Degree centrality is particularly useful for identifying the most central or influential nodes based on direct relationships.

 The degree centrality \( C_d(i) \) for a node \( i \) is calculated as:
  
  \[
  C_d(i) = \text{deg}(i)
  \]
  Where \( \text{deg}(i) \) is the number of edges connected to node \( i \).

\subsubsection{PageRank Centrality}

PageRank, developed by Google to rank web pages, measures the influence of a node by considering both the number and quality (weight) of its incoming edges. In the context of academic networks, PageRank helps identify authoritative nodes, where a high PageRank centrality indicates that a node is not only well-connected but also highly regarded by other influential nodes.

PageRank centrality \( PR(i) \) for node \( i \) is computed iteratively based on the network structure:
  
  \[
  PR(i) = \frac{1 - d}{N} + d \sum_{j \in \mathcal{N}(i)} \frac{PR(j)}{|\mathcal{N}(j)|}
  \]
  Where: 
  \( d \) is the damping factor (usually set to 0.85),
  \( N \) is the total number of nodes in the network,
  \( \mathcal{N}(i) \) is the set of neighbors of node \( i \),
  \( |\mathcal{N}(j)| \) is the number of outgoing edges from node \( j \).

\subsubsection{Why Degree Centrality and PageRank Centrality?}

The selection of Degree Centrality and PageRank Centrality as centrality measures is based on their ability to capture different yet complementary aspects of node influence and importance within a network.

\begin{itemize}
    \item Degree Centrality measures the number of direct connections a node has. In academic networks, it identifies the most connected authors or papers, making it useful for spotting collaboration hubs and active contributors.
    \item PageRank Centrality considers both the number and quality of connections, giving more importance to nodes linked to influential ones. This helps identify authoritative figures and key players in the network.
    \item 
\end{itemize}

Together, these measures provide a well-rounded view of node influence, capturing both direct connectivity (Degree) and overall importance (PageRank).


\subsection{Community Diameters}

The diameter of a community refers to the longest shortest path between any two nodes within that community. A smaller diameter indicates that nodes within the community are relatively close to one another, suggesting a more cohesive community. In contrast, a larger diameter may indicate the presence of disconnected subgroups or more sparse relationships between community members.

\section{Link Prediction}
Traditional link prediction methods for graph data mainly rely on structural features and similarity metrics of the graph. By calculating the similarity between nodes, utilizing path information, or applying statistical models, these methods predict potential links. They are simple and efficient, suitable for various types of graph data. Although they may face challenges in computational efficiency and accuracy when dealing with large and complex graphs, they lay the foundation for more advanced machine learning and deep learning-based approaches.

\subsection{Similarity-Based Metrics}

These methods predict potential links by calculating similarity scores between pairs of nodes. Common similarity metrics include:

\subsubsection{Common Neighbors (CN)}
Measures the number of shared neighbors between two nodes. The more common neighbors two nodes have, the higher the likelihood of a link forming between them.

\[
\text{CN}(x, y) = |\Gamma(x) \cap \Gamma(y)|
\]

where \( \Gamma(x) \) denotes the set of neighbors of node \( x \).

\subsubsection{Jaccard Coefficient}
Measures the ratio of the intersection to the union of the neighbor sets of two nodes, with values ranging from 0 to 1.

\[
J(x, y) = \frac{|\Gamma(x) \cap \Gamma(y)|}{|\Gamma(x) \cup \Gamma(y)|}
\]

\subsection{Introduction of GLACE}
    With the advancement of deep learning technologies, neural network-based models have shown remarkable performance in link prediction tasks. This report introduces the GLACE (Gaussian Latent Attribute-based Contrastive Embedding) model and its application in link prediction. The GLACE model is a Gaussian-based graph embedding method designed for link prediction tasks. Unlike the LACE model, GLACE learns Gaussian distribution embeddings (mean $\mu$ and variance $\sigma$) for each node, enabling it to better capture the uncertainty and complex relationships between nodes. The model minimizes the symmetric Kullback-Leibler (KL) divergence between node pairs, aligning their distributions in the embedding space to enhance link prediction accuracy.

\subsection{Model Architecture}

The main components of the GLACE model include:

\begin{itemize}
    \item \textbf{Input Processing}: Handles the sparse adjacency matrix by converting it into a format suitable for PyTorch sparse tensors.
    \item \textbf{Encoder}: A multi-layer fully connected neural network that extracts latent features of nodes.
    \item \textbf{Mean and Variance Embedding Layers}: Linear layers that generate the mean $\mu$ and log variance $\log\sigma$ for node embeddings.
    \item \textbf{Context Encoder}: Used when considering second-order proximity, it generates context embeddings for nodes.
    \item \textbf{Optimizer}: Utilizes the Adam optimizer for training the model parameters.
\end{itemize}

\subsection{Gaussian Embeddings and KL Divergence}

GLACE learns Gaussian distribution embeddings for each node, represented as $(\mu, \sigma)$. During link prediction, the symmetric KL divergence between node pairs is computed to measure their similarity. The specific steps are as follows:

\begin{enumerate}
    \item For a node pair $(u_i, u_j)$, retrieve their means $\mu_i, \mu_j$ and variances $\sigma_i, \sigma_j$.
    \item Calculate the KL divergence $KL(P||Q)$ and $KL(Q||P)$, where $P$ and $Q$ represent the Gaussian distributions of nodes $u_i$ and $u_j$, respectively.
    \item Average the two divergences to obtain the distance metric $KL\_distance$ between the node pair.
\end{enumerate}

\subsection{Model Training and Optimization}

The training process of the GLACE model involves several key steps:

\subsubsection{Loss Function}

The model employs a log-sigmoid loss function, suitable for binary classification tasks in link prediction. It is defined as:

\[
\text{Loss} = -\frac{1}{N} \sum_{i=1}^{N} \log \sigma(label_i \cdot energy_i)
\]

where $label_i$ is the true label (1 for positive samples, -1 for negative samples), and $energy_i$ is the energy value computed by the model (negative KL divergence).

\subsubsection{Optimization Process}

The Adam optimizer is used to update the model parameters, with the learning rate specified by the experimental setup. The optimization goal is to minimize the loss function, thereby improving the model's performance in link prediction tasks.

\subsubsection{Experimental Results}

\paragraph{Dataset}
For our experiments, we utilized the \textbf{Cora\_ML} dataset, a widely recognized benchmark in the field of link prediction and graph-based learning. The Cora\_ML dataset consists of scientific publications classified into various topics, with citation links representing the relationships between these publications. Specifically, the dataset contains 2,708 nodes (publications), 5,429 edges (citations), and 1,433 features representing the presence of specific words in the documents. This dataset is well-suited for evaluating the performance of graph embedding models like GLACE in predicting missing or potential links within the citation network.

\paragraph{Results}
The GLACE model was trained and evaluated on the Cora\_ML dataset over multiple batches. The key performance metrics recorded during the training process include loss, validation AUC (Area Under the Curve), and validation AP (Average Precision). Table \ref{tab:experimental_results} presents a summarized view of the results across different training batches, with intermediate batches omitted for brevity.

\begin{table}[h]
    \centering
    \caption{GLACE Model Performance on Cora\_ML Dataset}
    \label{tab:experimental_results}
    \begin{tabular}{cccc}
        \hline
        \textbf{Batch} & \textbf{Loss} & \textbf{Val AUC} & \textbf{Val AP} \\
        \hline
        49    & 0.303630 & 0.849437 & 0.828072 \\
        99    & 0.258379 & 0.891329 & 0.872213 \\
        149   & 0.266745 & 0.910746 & 0.896074 \\
        \vdots & \vdots   & \vdots    & \vdots    \\
        1749  & 0.172694 & 0.952421 & 0.949143 \\
        1799  & 0.182493 & 0.953662 & 0.948588 \\
        1849  & 0.207456 & 0.954621 & 0.950908 \\
        \hline
    \end{tabular}
\end{table}

\paragraph{Analysis of Results}
The experimental results on the Cora\_ML dataset demonstrate the effectiveness of the GLACE model in link prediction tasks. As observed from Table \ref{tab:experimental_results}, several key trends emerge:

\begin{itemize}
    \item \textbf{Loss Reduction}: The loss consistently decreases as training progresses, indicating that the model is effectively learning to minimize the discrepancy between predicted and actual links. For instance, the loss decreased from 0.303630 at batch 49 to 0.172694 at batch 1749.
    
    \item \textbf{Performance Metrics}: Both validation AUC and validation AP show an overall upward trend, reaching values above 0.95 towards the later batches. This signifies that the model's ability to distinguish between positive and negative links improves with training. For example, the validation AUC increased from 0.849437 at batch 49 to 0.954621 at batch 1849, and the validation AP similarly rose from 0.828072 to 0.950908.
    
    \item \textbf{Stability of Training}: The training process progresses smoothly without significant interruptions or delays, ensuring a steady training flow. The consistent decrease in loss and increase in performance metrics reflect the model's stable and effective learning dynamics.
\end{itemize}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{img/link_prediction.jpg}
    \caption{GLACE's link prediction performance}
    \label{fig:enter-label}
\end{figure}
Overall, the GLACE model exhibits robust performance in link prediction on the Cora\_ML dataset, achieving high accuracy and precision. The incorporation of Gaussian embeddings allows the model to capture nuanced relationships between nodes, resulting in superior predictive capabilities compared to traditional methods.

\section{Visualization System}

This study introduces a comprehensive visualization system designed to represent a paper citation network, where nodes correspond to academic papers and edges represent citations. The system is organized into five main sections: community network, bibliometric data, degree distribution, node distribution, and additional metrics. An overview of the system is shown in Figure \ref{fig:visualize}.

Given the modularity of author communities (0.66) and paper communities (0.99), the focus is placed on paper-level data, which offers a clearer and more structured representation of the citation network. The higher modularity of paper communities facilitates more detailed analysis. For optimal visualization, data is derived from the top 10 paper communities and the 50 highest centrality papers, ensuring the most influential communities and papers are prominently displayed. This approach enhances both clarity and effectiveness.

\subsection{Top Left: Community Network Visualization with Interactive Features}

The top-left section visualizes the paper communities using an interactive network graph. Nodes represent individual papers, and edges indicate citations. Users can zoom, pan, and select nodes, which dynamically updates the corresponding sections (top-right and bottom-middle) with community-specific data. A year slider allows users to track the temporal evolution of the citation network, highlighting changes in structure over time.

\subsection{Top Right: Bibliometric Data Table}

The top-right section presents a dynamic bibliometric table containing key attributes such as paper ID, publication year, venue, references, and citation count. This table updates in response to node selections from the network graph, ensuring that users view relevant bibliometric data for the selected community. Pagination and dynamic updates allow efficient browsing of large datasets.

\subsection{Bottom Left: Dynamic Bar Chart Visualization}

The bottom-left section features a dynamic bar chart for metrics such as "Average Citations," "Average Centrality," and "Diameter." Users can select a metric from a dropdown menu, and the bar chart visually compares these metrics across communities. The chart includes smooth transitions, tooltips, and highlighted bars on hover, enhancing interactivity.

\subsection{Bottom Middle: Degree Distribution Visualization}

The bottom-middle section visualizes the degree distribution of communities within the network using a smooth line chart with spline curves. This chart depicts the frequency of nodes with each degree. Upon selecting a node in the network graph, the degree distribution for the corresponding community is displayed dynamically. Tooltips and adjustable dot sizes further improve visibility, and degree values exceeding a specified threshold are grouped for clarity.

\subsection{Bottom Right: Node Distribution with Donut Chart}

The bottom-right section uses a donut chart to show the distribution of nodes across communities. Each slice represents a community, with its size proportional to the number of nodes. Interactive features such as hover effects and percentage labels allow users to explore the relative sizes of the communities and gain insights into the network's composition.

\subsection{System Overview and Interactivity}

The visualization system integrates these components into a cohesive interface that enables detailed exploration of the citation network. Key interactive features include zooming, panning, node selection, and year slider adjustments. Selecting a node updates the bibliometric data and degree distribution sections with community-specific information. The year slider adds a temporal dimension, revealing the evolution of the network over time. Powered by the \texttt{D3.js} library, the system ensures responsive, high-quality visualizations and an engaging user experience.


\section{Conclusion}

\section*{Acknowledgements}


% % Entries for the entire Anthology, followed by custom entries
% \bibliography{anthology,custom}
% \bibliographystyle{acl_natbib}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is a section in the appendix.

\end{document}
